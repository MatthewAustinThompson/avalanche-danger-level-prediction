{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizing Models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# sklearn \n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, \\\n",
    "    validation_curve\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, \\\n",
    "    precision_recall_curve, roc_curve, roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.combine import SMOTETomek"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>avi_danger</th>\n",
       "      <th>avg_wind</th>\n",
       "      <th>temp_max_swing</th>\n",
       "      <th>temp_max_swing_from_avg</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>temp_max</th>\n",
       "      <th>temp_min</th>\n",
       "      <th>...</th>\n",
       "      <th>prevailing_wind_N_2</th>\n",
       "      <th>prevailing_wind_NE_2</th>\n",
       "      <th>prevailing_wind_NW_2</th>\n",
       "      <th>prevailing_wind_S_2</th>\n",
       "      <th>prevailing_wind_SE_2</th>\n",
       "      <th>prevailing_wind_SW_2</th>\n",
       "      <th>prevailing_wind_W_2</th>\n",
       "      <th>three_day_snow_2</th>\n",
       "      <th>five_day_snow_2</th>\n",
       "      <th>next_day_avi_danger</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20.58</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>15</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>35.12</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>18</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.3</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>33.78</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>15</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>31.32</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>15</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.3</td>\n",
       "      <td>4.5</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>32.44</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>17</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.3</td>\n",
       "      <td>8.6</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 74 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  avi_danger  avg_wind  temp_max_swing  temp_max_swing_from_avg  \\\n",
       "0           0         1.0     20.58             0.0                      0.0   \n",
       "1           1         3.0     35.12             3.0                      0.0   \n",
       "2           2         2.0     33.78            -3.0                      0.0   \n",
       "3           3         3.0     31.32             0.0                      0.0   \n",
       "4           4         2.0     32.44             2.0                      1.4   \n",
       "\n",
       "     year  month   day  temp_max  temp_min  ...  prevailing_wind_N_2  \\\n",
       "0  2010.0   12.0  20.0        15         5  ...                  0.0   \n",
       "1  2010.0   12.0  21.0        18        10  ...                  0.0   \n",
       "2  2010.0   12.0  22.0        15         7  ...                  1.0   \n",
       "3  2010.0   12.0  23.0        15         6  ...                  1.0   \n",
       "4  2010.0   12.0  24.0        17         9  ...                  1.0   \n",
       "\n",
       "   prevailing_wind_NE_2  prevailing_wind_NW_2  prevailing_wind_S_2  \\\n",
       "0                   0.0                   1.0                  0.0   \n",
       "1                   0.0                   1.0                  0.0   \n",
       "2                   0.0                   0.0                  0.0   \n",
       "3                   0.0                   0.0                  0.0   \n",
       "4                   0.0                   0.0                  0.0   \n",
       "\n",
       "   prevailing_wind_SE_2  prevailing_wind_SW_2  prevailing_wind_W_2  \\\n",
       "0                   0.0                   0.0                  0.0   \n",
       "1                   0.0                   0.0                  0.0   \n",
       "2                   0.0                   0.0                  0.0   \n",
       "3                   0.0                   0.0                  0.0   \n",
       "4                   0.0                   0.0                  0.0   \n",
       "\n",
       "   three_day_snow_2  five_day_snow_2  next_day_avi_danger  \n",
       "0               0.2              0.2                  3.0  \n",
       "1               0.3              0.3                  2.0  \n",
       "2               2.5              2.5                  3.0  \n",
       "3               4.3              4.5                  2.0  \n",
       "4               8.3              8.6                  2.0  \n",
       "\n",
       "[5 rows x 74 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avi = pd.read_csv('SnowWeatherCleanFE.csv')\n",
    "avi.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1224 entries, 0 to 1223\n",
      "Data columns (total 74 columns):\n",
      " #   Column                     Non-Null Count  Dtype  \n",
      "---  ------                     --------------  -----  \n",
      " 0   Unnamed: 0                 1224 non-null   int64  \n",
      " 1   avi_danger                 1224 non-null   float64\n",
      " 2   avg_wind                   1224 non-null   float64\n",
      " 3   temp_max_swing             1224 non-null   float64\n",
      " 4   temp_max_swing_from_avg    1224 non-null   float64\n",
      " 5   year                       1224 non-null   float64\n",
      " 6   month                      1224 non-null   float64\n",
      " 7   day                        1224 non-null   float64\n",
      " 8   temp_max                   1224 non-null   int64  \n",
      " 9   temp_min                   1224 non-null   int64  \n",
      " 10  water_equivalent           1224 non-null   float64\n",
      " 11  snow_fall                  1224 non-null   float64\n",
      " 12  snow_depth_6am             1224 non-null   float64\n",
      " 13  wind_speed_sum             1224 non-null   int64  \n",
      " 14  sunshine_percent           1224 non-null   int64  \n",
      " 15  west_wind_hours            1224 non-null   int64  \n",
      " 16  northwest_wind_hours       1224 non-null   int64  \n",
      " 17  prevailing_wind_E          1224 non-null   int64  \n",
      " 18  prevailing_wind_N          1224 non-null   int64  \n",
      " 19  prevailing_wind_NE         1224 non-null   int64  \n",
      " 20  prevailing_wind_NW         1224 non-null   int64  \n",
      " 21  prevailing_wind_S          1224 non-null   int64  \n",
      " 22  prevailing_wind_SE         1224 non-null   int64  \n",
      " 23  prevailing_wind_SW         1224 non-null   int64  \n",
      " 24  prevailing_wind_W          1224 non-null   int64  \n",
      " 25  three_day_snow             1224 non-null   float64\n",
      " 26  five_day_snow              1224 non-null   float64\n",
      " 27  avi_danger_1               1224 non-null   float64\n",
      " 28  avg_wind_1                 1224 non-null   float64\n",
      " 29  temp_max_swing_1           1224 non-null   float64\n",
      " 30  temp_max_swing_from_avg_1  1224 non-null   float64\n",
      " 31  temp_max_1                 1224 non-null   float64\n",
      " 32  temp_min_1                 1224 non-null   float64\n",
      " 33  water_equivalent_1         1224 non-null   float64\n",
      " 34  snow_fall_1                1224 non-null   float64\n",
      " 35  snow_depth_6am_1           1224 non-null   float64\n",
      " 36  wind_speed_sum_1           1224 non-null   float64\n",
      " 37  sunshine_percent_1         1224 non-null   float64\n",
      " 38  west_wind_hours_1          1224 non-null   float64\n",
      " 39  northwest_wind_hours_1     1224 non-null   float64\n",
      " 40  prevailing_wind_E_1        1224 non-null   float64\n",
      " 41  prevailing_wind_N_1        1224 non-null   float64\n",
      " 42  prevailing_wind_NE_1       1224 non-null   float64\n",
      " 43  prevailing_wind_NW_1       1224 non-null   float64\n",
      " 44  prevailing_wind_S_1        1224 non-null   float64\n",
      " 45  prevailing_wind_SE_1       1224 non-null   float64\n",
      " 46  prevailing_wind_SW_1       1224 non-null   float64\n",
      " 47  prevailing_wind_W_1        1224 non-null   float64\n",
      " 48  three_day_snow_1           1224 non-null   float64\n",
      " 49  five_day_snow_1            1224 non-null   float64\n",
      " 50  avi_danger_2               1224 non-null   float64\n",
      " 51  avg_wind_2                 1224 non-null   float64\n",
      " 52  temp_max_swing_2           1224 non-null   float64\n",
      " 53  temp_max_swing_from_avg_2  1224 non-null   float64\n",
      " 54  temp_max_2                 1224 non-null   float64\n",
      " 55  temp_min_2                 1224 non-null   float64\n",
      " 56  water_equivalent_2         1224 non-null   float64\n",
      " 57  snow_fall_2                1224 non-null   float64\n",
      " 58  snow_depth_6am_2           1224 non-null   float64\n",
      " 59  wind_speed_sum_2           1224 non-null   float64\n",
      " 60  sunshine_percent_2         1224 non-null   float64\n",
      " 61  west_wind_hours_2          1224 non-null   float64\n",
      " 62  northwest_wind_hours_2     1224 non-null   float64\n",
      " 63  prevailing_wind_E_2        1224 non-null   float64\n",
      " 64  prevailing_wind_N_2        1224 non-null   float64\n",
      " 65  prevailing_wind_NE_2       1224 non-null   float64\n",
      " 66  prevailing_wind_NW_2       1224 non-null   float64\n",
      " 67  prevailing_wind_S_2        1224 non-null   float64\n",
      " 68  prevailing_wind_SE_2       1224 non-null   float64\n",
      " 69  prevailing_wind_SW_2       1224 non-null   float64\n",
      " 70  prevailing_wind_W_2        1224 non-null   float64\n",
      " 71  three_day_snow_2           1224 non-null   float64\n",
      " 72  five_day_snow_2            1224 non-null   float64\n",
      " 73  next_day_avi_danger        1224 non-null   float64\n",
      "dtypes: float64(59), int64(15)\n",
      "memory usage: 707.8 KB\n"
     ]
    }
   ],
   "source": [
    "avi.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### filter data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "avi = avi.drop(['Unnamed: 0'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# avi = avi.drop(['prevailing_wind_E', 'prevailing_wind_N', 'prevailing_wind_NE', 'prevailing_wind_NW',\n",
    "#                 'prevailing_wind_S', 'prevailing_wind_SE', 'prevailing_wind_SW', 'prevailing_wind_W',\n",
    "#                 'prevailing_wind_E_1', 'prevailing_wind_N_1', 'prevailing_wind_NE_1', 'prevailing_wind_NW_1',\n",
    "#                 'prevailing_wind_S_1', 'prevailing_wind_SE_1', 'prevailing_wind_SW_1', 'prevailing_wind_W_1',\n",
    "#                 'prevailing_wind_E_2', 'prevailing_wind_N_2', 'prevailing_wind_NE_2', 'prevailing_wind_NW_2',\n",
    "#                 'prevailing_wind_S_2', 'prevailing_wind_SE_2', 'prevailing_wind_SW_2', 'prevailing_wind_W_2'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1224, 72)\n",
      "(1224,)\n"
     ]
    }
   ],
   "source": [
    "x = avi.iloc[:, 0:avi.shape[1]-1]\n",
    "y = avi.iloc[:, avi.shape[1]-1]\n",
    "\n",
    "print(x.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Class Weights Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weight = class_weight.compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 0.8861003861003861, 2: 0.6428571428571429, 3: 0.9405737704918032, 4: 3.956896551724138}\n"
     ]
    }
   ],
   "source": [
    "weight_dict = {}\n",
    "for class_num in range(0, len(class_weight)):\n",
    "    weight_dict[class_num + 1] = class_weight[class_num]\n",
    "print(weight_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standardizing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#standardizer \n",
    "def standardize(X_train, X_test):\n",
    "    scaler = StandardScaler()\n",
    "    # Fitting and transforming training data\n",
    "    scaler.fit(X_train)\n",
    "    X_train = scaler.transform(X_train)\n",
    "    # Tranforming testing data based on traning fit (prevent data leakage)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test = standardize(x_train, x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30      3.0\n",
      "242     1.0\n",
      "465     1.0\n",
      "136     3.0\n",
      "1031    1.0\n",
      "       ... \n",
      "1044    2.0\n",
      "1095    1.0\n",
      "1130    1.0\n",
      "860     1.0\n",
      "1126    1.0\n",
      "Name: next_day_avi_danger, Length: 918, dtype: float64\n",
      "[[  1. 259.]\n",
      " [  2. 357.]\n",
      " [  3. 244.]\n",
      " [  4.  58.]]\n"
     ]
    }
   ],
   "source": [
    "print(y_train)\n",
    "\n",
    "(unique, counts) = np.unique(y_train, return_counts=True)\n",
    "frequencies = np.asarray((unique, counts)).T\n",
    "\n",
    "print(frequencies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest w/ Class Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6274509803921569\n",
      "[0.64052288 0.58496732 0.64052288]\n"
     ]
    }
   ],
   "source": [
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "\n",
    "\n",
    "rf = RandomForestClassifier(class_weight=weight_dict)\n",
    "rf_cv = RandomizedSearchCV(estimator=rf, param_distributions=random_grid, n_iter=5, scoring='f1_weighted')\n",
    "rf_cv.fit(x_train, y_train)\n",
    "y_pred_rf = rf_cv.predict(x_test)\n",
    "print(accuracy_score(y_test, y_pred_rf))\n",
    "print(cross_val_score(rf, x_train, y_train, cv=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 800, 'min_samples_split': 10, 'min_samples_leaf': 4, 'max_features': 'sqrt', 'max_depth': None, 'bootstrap': False}\n"
     ]
    }
   ],
   "source": [
    "print(rf_cv.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[70 21  3  0]\n",
      " [11 79 21  1]\n",
      " [ 9 35 40  4]\n",
      " [ 0  1  8  3]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test, y_pred_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.78      0.74      0.76        94\n",
      "         2.0       0.58      0.71      0.64       112\n",
      "         3.0       0.56      0.45      0.50        88\n",
      "         4.0       0.38      0.25      0.30        12\n",
      "\n",
      "    accuracy                           0.63       306\n",
      "   macro avg       0.57      0.54      0.55       306\n",
      "weighted avg       0.63      0.63      0.62       306\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_scores, test_scores = validation_curve(RandomForestClassifier(rf_cv.best_params_),\n",
    "#                                              x_train,\n",
    "#                                              y_train,\n",
    "#                                              param_name=\"n_estimators\",\n",
    "#                                              param_range=param_range,\n",
    "#                                              cv=3,\n",
    "#                                              scoring=\"accuracy\",\n",
    "#                                              n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(param_range, train_mean, label=\"Training score\", color=\"black\")\n",
    "# plt.plot(param_range, test_mean, label=\"Cross-validation score\", color=\"dimgrey\")\n",
    "# plt.title(\"Validation Curve With Random Forest\")\n",
    "# plt.xlabel(\"Number Of Trees\")\n",
    "# plt.ylabel(\"Accuracy Score\")\n",
    "# plt.tight_layout()\n",
    "# plt.legend(loc=\"best\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_scores, test_scores = validation_curve(RandomForestClassifier(rf_cv.best_params_),\n",
    "#                                              x_train,\n",
    "#                                              y_train,\n",
    "#                                              param_name=\"max_depth\",\n",
    "#                                              param_range=param_range,\n",
    "#                                              cv=3,\n",
    "#                                              scoring=\"accuracy\",\n",
    "#                                              n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(param_range, train_mean, label=\"Training score\", color=\"black\")\n",
    "# plt.plot(param_range, test_mean, label=\"Cross-validation score\", color=\"dimgrey\")\n",
    "# plt.title(\"Validation Curve With Random Forest\")\n",
    "# plt.xlabel(\"Number Of Trees\")\n",
    "# plt.ylabel(\"Accuracy Score\")\n",
    "# plt.tight_layout()\n",
    "# plt.legend(loc=\"best\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest w/ SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = SMOTE(random_state=12, sampling_strategy={4:250})\n",
    "x_res, y_res = sm.fit_sample(x_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#smk = SMOTETomek(random_state=12, sampling_strategy=1.0)\n",
    "#x_res, y_res = smk.fit_sample(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6241830065359477\n",
      "[0.62091503 0.58496732 0.6503268 ]\n"
     ]
    }
   ],
   "source": [
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "rf_cv = RandomizedSearchCV(estimator=rf, param_distributions=random_grid, n_iter=5, scoring='f1_weighted')\n",
    "rf_cv.fit(x_res, y_res)\n",
    "y_pred_rf = rf_cv.predict(x_test)\n",
    "print(accuracy_score(y_test, y_pred_rf))\n",
    "print(cross_val_score(rf, x_train, y_train, cv=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 600, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 'sqrt', 'max_depth': 90, 'bootstrap': False}\n"
     ]
    }
   ],
   "source": [
    "print(rf_cv.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[66 24  3  1]\n",
      " [10 88 13  1]\n",
      " [ 7 41 33  7]\n",
      " [ 0  4  4  4]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test, y_pred_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.80      0.70      0.75        94\n",
      "         2.0       0.56      0.79      0.65       112\n",
      "         3.0       0.62      0.38      0.47        88\n",
      "         4.0       0.31      0.33      0.32        12\n",
      "\n",
      "    accuracy                           0.62       306\n",
      "   macro avg       0.57      0.55      0.55       306\n",
      "weighted avg       0.64      0.62      0.62       306\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6405228758169934\n",
      "[0.59150327 0.59477124 0.64052288]\n"
     ]
    }
   ],
   "source": [
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "\n",
    "et = ExtraTreesClassifier(class_weight = weight_dict)\n",
    "et_cv = RandomizedSearchCV(estimator=et, param_distributions=random_grid, n_iter=5, scoring='f1_weighted')\n",
    "et_cv.fit(x_train, y_train)\n",
    "y_pred_et = et_cv.predict(x_test)\n",
    "print(accuracy_score(y_test, y_pred_et))\n",
    "print(cross_val_score(et, x_train, y_train, cv=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 1000, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': 'auto', 'max_depth': None, 'bootstrap': True}\n"
     ]
    }
   ],
   "source": [
    "print(et_cv.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[71 19  4  0]\n",
      " [10 81 20  1]\n",
      " [10 31 42  5]\n",
      " [ 1  2  7  2]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test, y_pred_et))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.77      0.76      0.76        94\n",
      "         2.0       0.61      0.72      0.66       112\n",
      "         3.0       0.58      0.48      0.52        88\n",
      "         4.0       0.25      0.17      0.20        12\n",
      "\n",
      "    accuracy                           0.64       306\n",
      "   macro avg       0.55      0.53      0.54       306\n",
      "weighted avg       0.64      0.64      0.63       306\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred_et))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6045751633986928\n",
      "[0.61764706 0.58169935 0.61111111]\n"
     ]
    }
   ],
   "source": [
    "param_distributions = {'learning_rate':[0.15,0.1,0.05,0.01,0.005,0.001], \n",
    "                       'n_estimators':[100,250,500,750,1000,1250,1500,1750],\n",
    "                       'max_depth':[2,3,4,5,6,7] }\n",
    "\n",
    "gbc = GradientBoostingClassifier(n_estimators=500, max_depth=4, learning_rate=0.05)\n",
    "# gbc_cv = RandomizedSearchCV(estimator=gbc, param_distributions= param_distributions, n_iter=5, scoring='f1_weighted')\n",
    "gbc.fit(x_res, y_res)\n",
    "y_pred_gbc = gbc.predict(x_test)\n",
    "print(accuracy_score(y_test, y_pred_gbc))\n",
    "print(cross_val_score(gbc, x_train, y_train, cv=3))\n",
    "\n",
    "# print(gbc_cv.best_params_)\n",
    "# {'n_estimators': 500, 'max_depth': 4, 'learning_rate': 0.05}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[64 22  8  0]\n",
      " [ 9 78 22  3]\n",
      " [10 29 40  9]\n",
      " [ 1  3  5  3]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test, y_pred_gbc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.76      0.68      0.72        94\n",
      "         2.0       0.59      0.70      0.64       112\n",
      "         3.0       0.53      0.45      0.49        88\n",
      "         4.0       0.20      0.25      0.22        12\n",
      "\n",
      "    accuracy                           0.60       306\n",
      "   macro avg       0.52      0.52      0.52       306\n",
      "weighted avg       0.61      0.60      0.60       306\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred_gbc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision trees with class weights balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5294117647058824\n",
      "[0.49019608 0.50326797 0.47712418]\n",
      "{'min_samples_split': 5, 'min_samples_leaf': 1, 'max_depth': 10}\n"
     ]
    }
   ],
   "source": [
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "# Create the random grid\n",
    "random_grid = {'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "              }\n",
    "\n",
    "\n",
    "ds = DecisionTreeClassifier(class_weight = weight_dict)\n",
    "ds_cv = RandomizedSearchCV(estimator=ds, param_distributions=random_grid, n_iter=100, scoring='f1_weighted')\n",
    "ds_cv.fit(x_train, y_train)\n",
    "y_pred_ds = ds_cv.predict(x_test)\n",
    "print(accuracy_score(y_test, y_pred_ds))\n",
    "print(cross_val_score(ds, x_train, y_train, cv=3))\n",
    "print(ds_cv.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[55 13 24  2]\n",
      " [11 57 41  3]\n",
      " [12 23 46  7]\n",
      " [ 1  6  1  4]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test, y_pred_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.70      0.59      0.64        94\n",
      "         2.0       0.58      0.51      0.54       112\n",
      "         3.0       0.41      0.52      0.46        88\n",
      "         4.0       0.25      0.33      0.29        12\n",
      "\n",
      "    accuracy                           0.53       306\n",
      "   macro avg       0.48      0.49      0.48       306\n",
      "weighted avg       0.55      0.53      0.54       306\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred_ds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression with class weights balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5522875816993464\n",
      "[0.50653595 0.5620915  0.56535948]\n",
      "{'tol': 0.0001, 'solver': 'lbfgs', 'penalty': 'l2', 'C': 0.1}\n"
     ]
    }
   ],
   "source": [
    "penalty = ['l2']\n",
    "tol = [0.0001, 0.001, 0.01, 0.1]\n",
    "C = [0.001, 0.01, 0.1, 1, 10, 100, 1000] \n",
    "solver = ['newton-cg', 'lbfgs', 'sag']\n",
    "param_distributions = dict(penalty=penalty,\n",
    "                           tol=tol,\n",
    "                           C=C,\n",
    "                           solver = solver)\n",
    "\n",
    "\n",
    "lr = LogisticRegression(class_weight = weight_dict, max_iter = 10000)\n",
    "lr_cv = RandomizedSearchCV(estimator=lr, param_distributions= param_distributions, n_iter=5, scoring='f1_weighted')\n",
    "lr_cv.fit(x_train, y_train)\n",
    "y_pred_lr = lr_cv.predict(x_test)\n",
    "print(accuracy_score(y_test, y_pred_lr))\n",
    "print(cross_val_score(lr, x_train, y_train, cv=3))\n",
    "print(lr_cv.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVC with class weights balanced "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5620915032679739\n",
      "[0.56862745 0.56862745 0.64705882]\n",
      "{'kernel': 'rbf', 'gamma': 0.01, 'C': 10}\n"
     ]
    }
   ],
   "source": [
    "param_distributions = {'C': [0.1, 1, 10, 100, 1000],  \n",
    "              'gamma': [1, 0.1, 0.01, 0.001, 0.0001], \n",
    "              'kernel': ['rbf']}  \n",
    "\n",
    "\n",
    "svm = SVC(class_weight = weight_dict)\n",
    "svm_cv = RandomizedSearchCV(estimator=svm, param_distributions= param_distributions, n_iter=5, scoring='f1_weighted')\n",
    "svm_cv.fit(x_train, y_train)\n",
    "y_pred_svm = svm_cv.predict(x_test)\n",
    "print(accuracy_score(y_test, y_pred_svm))\n",
    "print(cross_val_score(svm, x_train, y_train, cv=3))\n",
    "print(svm_cv.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5620915032679739\n",
      "[0.53267974 0.51633987 0.54248366]\n",
      "{'p': 1, 'n_neighbors': 25, 'leaf_size': 21}\n"
     ]
    }
   ],
   "source": [
    "#List Hyperparameters that we want to tune.\n",
    "leaf_size = list(range(1,50))\n",
    "n_neighbors = list(range(1,30))\n",
    "p=[1,2]\n",
    "#Convert to dictionary\n",
    "param_distributions = dict(leaf_size=leaf_size, n_neighbors=n_neighbors, p=p)\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "knn_cv = RandomizedSearchCV(estimator=knn, param_distributions= param_distributions, n_iter=5, scoring='f1_weighted')\n",
    "knn_cv.fit(x_train, y_train)\n",
    "y_pred_knn = knn_cv.predict(x_test)\n",
    "print(accuracy_score(y_test, y_pred_knn))\n",
    "print(cross_val_score(knn, x_train, y_train, cv=3))\n",
    "print(knn_cv.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
